<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">.lst-kix_pb2bpaedok4i-3>li{counter-increment:lst-ctn-kix_pb2bpaedok4i-3}.lst-kix_pb2bpaedok4i-8>li:before{content:"" counter(lst-ctn-kix_pb2bpaedok4i-8,lower-roman) ". "}.lst-kix_pb2bpaedok4i-5>li:before{content:"" counter(lst-ctn-kix_pb2bpaedok4i-5,lower-roman) ". "}.lst-kix_pb2bpaedok4i-4>li:before{content:"" counter(lst-ctn-kix_pb2bpaedok4i-4,lower-latin) ". "}.lst-kix_pb2bpaedok4i-6>li:before{content:"" counter(lst-ctn-kix_pb2bpaedok4i-6,decimal) ". "}.lst-kix_pb2bpaedok4i-7>li:before{content:"" counter(lst-ctn-kix_pb2bpaedok4i-7,lower-latin) ". "}.lst-kix_pb2bpaedok4i-3>li:before{content:"" counter(lst-ctn-kix_pb2bpaedok4i-3,decimal) ". "}.lst-kix_pb2bpaedok4i-4>li{counter-increment:lst-ctn-kix_pb2bpaedok4i-4}ul.lst-kix_9acpcmg5880u-1{list-style-type:none}ul.lst-kix_9acpcmg5880u-2{list-style-type:none}ol.lst-kix_pb2bpaedok4i-5.start{counter-reset:lst-ctn-kix_pb2bpaedok4i-5 0}ul.lst-kix_9acpcmg5880u-0{list-style-type:none}ul.lst-kix_9acpcmg5880u-5{list-style-type:none}ul.lst-kix_9acpcmg5880u-6{list-style-type:none}ul.lst-kix_9acpcmg5880u-3{list-style-type:none}ul.lst-kix_9acpcmg5880u-4{list-style-type:none}ul.lst-kix_u94b9dfyyvn-3{list-style-type:none}ul.lst-kix_u94b9dfyyvn-2{list-style-type:none}ul.lst-kix_u94b9dfyyvn-1{list-style-type:none}ul.lst-kix_9acpcmg5880u-7{list-style-type:none}.lst-kix_pb2bpaedok4i-0>li:before{content:"" counter(lst-ctn-kix_pb2bpaedok4i-0,decimal) ". "}.lst-kix_pb2bpaedok4i-2>li:before{content:"" counter(lst-ctn-kix_pb2bpaedok4i-2,lower-roman) ". "}ul.lst-kix_u94b9dfyyvn-0{list-style-type:none}ul.lst-kix_9acpcmg5880u-8{list-style-type:none}ul.lst-kix_u94b9dfyyvn-7{list-style-type:none}ul.lst-kix_u94b9dfyyvn-6{list-style-type:none}ul.lst-kix_u94b9dfyyvn-5{list-style-type:none}.lst-kix_pb2bpaedok4i-1>li:before{content:"" counter(lst-ctn-kix_pb2bpaedok4i-1,lower-latin) ". "}ul.lst-kix_u94b9dfyyvn-4{list-style-type:none}ol.lst-kix_pb2bpaedok4i-8.start{counter-reset:lst-ctn-kix_pb2bpaedok4i-8 0}ul.lst-kix_u94b9dfyyvn-8{list-style-type:none}ul.lst-kix_clvlb54q34iz-4{list-style-type:none}ul.lst-kix_clvlb54q34iz-5{list-style-type:none}.lst-kix_git6csdi9890-6>li:before{content:"\0025cf  "}.lst-kix_git6csdi9890-7>li:before{content:"\0025cb  "}ul.lst-kix_clvlb54q34iz-6{list-style-type:none}ul.lst-kix_clvlb54q34iz-7{list-style-type:none}ul.lst-kix_clvlb54q34iz-0{list-style-type:none}ul.lst-kix_clvlb54q34iz-1{list-style-type:none}.lst-kix_git6csdi9890-5>li:before{content:"\0025a0  "}ul.lst-kix_clvlb54q34iz-2{list-style-type:none}ul.lst-kix_clvlb54q34iz-3{list-style-type:none}ol.lst-kix_pb2bpaedok4i-6.start{counter-reset:lst-ctn-kix_pb2bpaedok4i-6 0}ul.lst-kix_clvlb54q34iz-8{list-style-type:none}.lst-kix_git6csdi9890-8>li:before{content:"\0025a0  "}ul.lst-kix_a60e7wsgu2v7-5{list-style-type:none}ul.lst-kix_a60e7wsgu2v7-6{list-style-type:none}ul.lst-kix_a60e7wsgu2v7-7{list-style-type:none}ul.lst-kix_a60e7wsgu2v7-8{list-style-type:none}ol.lst-kix_pb2bpaedok4i-0.start{counter-reset:lst-ctn-kix_pb2bpaedok4i-0 0}ul.lst-kix_a60e7wsgu2v7-1{list-style-type:none}ul.lst-kix_a60e7wsgu2v7-2{list-style-type:none}ul.lst-kix_a60e7wsgu2v7-3{list-style-type:none}.lst-kix_pb2bpaedok4i-2>li{counter-increment:lst-ctn-kix_pb2bpaedok4i-2}ul.lst-kix_a60e7wsgu2v7-4{list-style-type:none}.lst-kix_pb2bpaedok4i-5>li{counter-increment:lst-ctn-kix_pb2bpaedok4i-5}ul.lst-kix_a60e7wsgu2v7-0{list-style-type:none}.lst-kix_a60e7wsgu2v7-5>li:before{content:"\0025a0  "}.lst-kix_a60e7wsgu2v7-7>li:before{content:"\0025cb  "}.lst-kix_a60e7wsgu2v7-6>li:before{content:"\0025cf  "}.lst-kix_a60e7wsgu2v7-8>li:before{content:"\0025a0  "}ul.lst-kix_git6csdi9890-5{list-style-type:none}ul.lst-kix_git6csdi9890-4{list-style-type:none}ul.lst-kix_git6csdi9890-3{list-style-type:none}ul.lst-kix_git6csdi9890-2{list-style-type:none}ol.lst-kix_pb2bpaedok4i-4.start{counter-reset:lst-ctn-kix_pb2bpaedok4i-4 0}ul.lst-kix_git6csdi9890-8{list-style-type:none}ul.lst-kix_git6csdi9890-7{list-style-type:none}ul.lst-kix_git6csdi9890-6{list-style-type:none}ol.lst-kix_pb2bpaedok4i-1.start{counter-reset:lst-ctn-kix_pb2bpaedok4i-1 0}.lst-kix_a60e7wsgu2v7-4>li:before{content:"\0025cb  "}.lst-kix_u94b9dfyyvn-7>li:before{content:"\0025cb  "}.lst-kix_pb2bpaedok4i-8>li{counter-increment:lst-ctn-kix_pb2bpaedok4i-8}ol.lst-kix_pb2bpaedok4i-7.start{counter-reset:lst-ctn-kix_pb2bpaedok4i-7 0}.lst-kix_a60e7wsgu2v7-3>li:before{content:"\0025cf  "}.lst-kix_u94b9dfyyvn-8>li:before{content:"\0025a0  "}.lst-kix_a60e7wsgu2v7-2>li:before{content:"\0025a0  "}.lst-kix_a60e7wsgu2v7-1>li:before{content:"\0025cb  "}ul.lst-kix_git6csdi9890-1{list-style-type:none}ul.lst-kix_git6csdi9890-0{list-style-type:none}.lst-kix_a60e7wsgu2v7-0>li:before{content:"\0025cf  "}.lst-kix_u94b9dfyyvn-0>li:before{content:"\0025cf  "}.lst-kix_9acpcmg5880u-0>li:before{content:"\0025cf  "}.lst-kix_u94b9dfyyvn-5>li:before{content:"\0025a0  "}.lst-kix_u94b9dfyyvn-6>li:before{content:"\0025cf  "}.lst-kix_9acpcmg5880u-1>li:before{content:"\0025cb  "}.lst-kix_9acpcmg5880u-4>li:before{content:"\0025cb  "}.lst-kix_clvlb54q34iz-6>li:before{content:"\0025cf  "}.lst-kix_clvlb54q34iz-7>li:before{content:"\0025cb  "}.lst-kix_9acpcmg5880u-2>li:before{content:"\0025a0  "}.lst-kix_9acpcmg5880u-6>li:before{content:"\0025cf  "}.lst-kix_clvlb54q34iz-4>li:before{content:"\0025cb  "}.lst-kix_clvlb54q34iz-5>li:before{content:"\0025a0  "}.lst-kix_clvlb54q34iz-8>li:before{content:"\0025a0  "}.lst-kix_git6csdi9890-1>li:before{content:"\0025cb  "}.lst-kix_u94b9dfyyvn-4>li:before{content:"\0025cb  "}.lst-kix_9acpcmg5880u-3>li:before{content:"\0025cf  "}.lst-kix_9acpcmg5880u-7>li:before{content:"\0025cb  "}.lst-kix_u94b9dfyyvn-3>li:before{content:"\0025cf  "}.lst-kix_git6csdi9890-2>li:before{content:"\0025a0  "}.lst-kix_git6csdi9890-3>li:before{content:"\0025cf  "}.lst-kix_u94b9dfyyvn-1>li:before{content:"\0025cb  "}.lst-kix_git6csdi9890-4>li:before{content:"\0025cb  "}.lst-kix_u94b9dfyyvn-2>li:before{content:"\0025a0  "}.lst-kix_9acpcmg5880u-5>li:before{content:"\0025a0  "}.lst-kix_pb2bpaedok4i-0>li{counter-increment:lst-ctn-kix_pb2bpaedok4i-0}.lst-kix_pb2bpaedok4i-6>li{counter-increment:lst-ctn-kix_pb2bpaedok4i-6}.lst-kix_clvlb54q34iz-0>li:before{content:"\0025cf  "}.lst-kix_clvlb54q34iz-1>li:before{content:"\0025cb  "}.lst-kix_9acpcmg5880u-8>li:before{content:"\0025a0  "}.lst-kix_clvlb54q34iz-2>li:before{content:"\0025a0  "}.lst-kix_clvlb54q34iz-3>li:before{content:"\0025cf  "}.lst-kix_git6csdi9890-0>li:before{content:"\0025cf  "}.lst-kix_1g08cjbhfinr-0>li:before{content:"\0025cf  "}.lst-kix_1g08cjbhfinr-2>li:before{content:"\0025a0  "}ol.lst-kix_pb2bpaedok4i-2{list-style-type:none}ol.lst-kix_pb2bpaedok4i-3{list-style-type:none}ol.lst-kix_pb2bpaedok4i-0{list-style-type:none}ol.lst-kix_pb2bpaedok4i-2.start{counter-reset:lst-ctn-kix_pb2bpaedok4i-2 0}.lst-kix_1g08cjbhfinr-1>li:before{content:"\0025cb  "}ol.lst-kix_pb2bpaedok4i-1{list-style-type:none}.lst-kix_1g08cjbhfinr-4>li:before{content:"\0025cb  "}.lst-kix_1g08cjbhfinr-3>li:before{content:"\0025cf  "}.lst-kix_pb2bpaedok4i-1>li{counter-increment:lst-ctn-kix_pb2bpaedok4i-1}.lst-kix_1g08cjbhfinr-8>li:before{content:"\0025a0  "}.lst-kix_1g08cjbhfinr-5>li:before{content:"\0025a0  "}ol.lst-kix_pb2bpaedok4i-6{list-style-type:none}.lst-kix_pb2bpaedok4i-7>li{counter-increment:lst-ctn-kix_pb2bpaedok4i-7}ol.lst-kix_pb2bpaedok4i-7{list-style-type:none}ol.lst-kix_pb2bpaedok4i-4{list-style-type:none}.lst-kix_1g08cjbhfinr-6>li:before{content:"\0025cf  "}ol.lst-kix_pb2bpaedok4i-5{list-style-type:none}ol.lst-kix_pb2bpaedok4i-8{list-style-type:none}.lst-kix_1g08cjbhfinr-7>li:before{content:"\0025cb  "}ul.lst-kix_1g08cjbhfinr-2{list-style-type:none}ul.lst-kix_1g08cjbhfinr-1{list-style-type:none}ul.lst-kix_1g08cjbhfinr-0{list-style-type:none}ul.lst-kix_1g08cjbhfinr-6{list-style-type:none}ul.lst-kix_1g08cjbhfinr-5{list-style-type:none}ul.lst-kix_1g08cjbhfinr-4{list-style-type:none}ul.lst-kix_1g08cjbhfinr-3{list-style-type:none}ol.lst-kix_pb2bpaedok4i-3.start{counter-reset:lst-ctn-kix_pb2bpaedok4i-3 0}ul.lst-kix_1g08cjbhfinr-8{list-style-type:none}ul.lst-kix_1g08cjbhfinr-7{list-style-type:none}ol{margin:0;padding:0}table td,table th{padding:0}.c6{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:92.2pt;border-top-color:#000000;border-bottom-style:solid}.c13{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:159.8pt;border-top-color:#000000;border-bottom-style:solid}.c1{-webkit-text-decoration-skip:none;color:#000000;font-weight:700;text-decoration:underline;vertical-align:baseline;text-decoration-skip-ink:none;font-size:11pt;font-family:"Times New Roman";font-style:italic}.c14{margin-left:36pt;padding-top:12pt;padding-left:0pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c16{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c8{color:#000000;font-weight:700;vertical-align:baseline;font-size:11pt;font-family:"Times New Roman";font-style:normal}.c19{color:#202124;text-decoration:none;vertical-align:baseline;font-size:12pt;font-style:normal}.c5{color:#000000;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c11{color:#000000;text-decoration:none;vertical-align:baseline;font-size:10pt;font-style:normal}.c30{color:#202124;text-decoration:none;vertical-align:baseline;font-size:11pt;font-style:normal}.c0{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c25{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c26{border-spacing:0;border-collapse:collapse;margin-right:auto}.c9{color:#000000;vertical-align:baseline;font-size:11pt;font-style:normal}.c4{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;text-decoration:underline}.c10{font-weight:700;font-family:"Times New Roman";font-style:italic}.c2{font-weight:400;font-family:"Times New Roman"}.c17{color:inherit;text-decoration:inherit}.c28{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c7{font-size:12pt;font-style:italic}.c22{margin-left:36pt;padding-left:0pt}.c20{padding:0;margin:0}.c23{margin-left:36pt}.c31{vertical-align:sub}.c21{height:0pt}.c24{font-size:12pt}.c18{background-color:#ffffff}.c27{color:#1155cc}.c15{height:11pt}.c29{height:21.6pt}.c12{font-style:italic}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body><div class="c18 c28"><p class="c3"><span class="c4 c10">Introduction/Background</span></p><p class="c3"><span class="c2">When listening to music, people often wonder what instrument is playing. The goal of this project is to create a model to predict the instrument playing the music in a given sound bite. This is similar to the popular app Shazam, but instead of returning what song is playing (as Shazam does), our program would return the instrument that is playing the music heard in the audio file.</span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3"><span class="c4 c10">Problem Definition</span></p><p class="c3"><span class="c2">Given an audio file of a single instrument playing, would it be possible to develop a program based on machine learning to determine what instrument is playing the music heard in the file?</span></p><p class="c3"><span class="c2"><br></span><span class="c4 c10">Data Collection</span></p><p class="c3"><span class="c5 c2">We obtained our training, validation, and testing data from TensorFlow&rsquo;s NSynth Dataset. The Dataset is composed of over 300,000 samples of 4-second .wav files (sampling rate of 22050 Hz) with their corresponding annotations. Each of the annotations contains an instrument family label ranging from 0 to 10; each number corresponds to an instrument family.</span></p><p class="c3 c15"><span class="c5 c2"></span></p><a id="t.f47e46740831117dab310d05084238e1799f1843"></a><a id="t.0"></a><table class="c26"><tbody><tr class="c21"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">Label Number</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">Instrument Family</span></p></td></tr><tr class="c21"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">0</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">bass</span></p></td></tr><tr class="c21"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">1</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">brass</span></p></td></tr><tr class="c21"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">2</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">flute</span></p></td></tr><tr class="c21"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">3</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">guitar</span></p></td></tr><tr class="c21"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">4</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">keyboard</span></p></td></tr><tr class="c29"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">5</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">mallet</span></p></td></tr><tr class="c21"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">6</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">organ</span></p></td></tr><tr class="c21"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">7</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">reed</span></p></td></tr><tr class="c21"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">8</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">string</span></p></td></tr><tr class="c21"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">9</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">synth_lead</span></p></td></tr><tr class="c21"><td class="c6" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">10</span></p></td><td class="c13" colspan="1" rowspan="1"><p class="c0"><span class="c5 c2">vocal</span></p></td></tr></tbody></table><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3"><span class="c5 c2">Due to limitations in our computing power, we decided to only download and use 12,678 samples in the first version of our model. We also downloaded a JSON object containing the annotations for each .wav sample file. Once downloaded, we can obtain the instrument family label corresponding to each .wav file by parsing the JSON object.</span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3"><span class="c2">In addition, we also need to process the .wav sound files before we can effectively run the data through our feature extraction methods. Each .wav file can be loaded into Python as a 1-D array of sound amplitudes using the </span><span class="c2 c12">librosa </span><span class="c5 c2">library. Each .wav file contains data for 4 seconds of audio, but often the instrument played in the file does not last for 4 seconds, resulting in a brief silence at the end of the 4 seconds. This silence interferes with some of the methods we are implementing for feature extraction (described in the next section). Thus, as part of the data cleaning process, we deleted any trailing zeros in the 1-D array, so that we only process the data representing an instrument playing.</span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3"><span class="c4 c10">Methods</span><span class="c4 c2">&nbsp;</span><span class="c4 c10">Part 1: Feature Extraction</span></p><p class="c3"><span class="c2">A 1-D array of sound amplitudes is used to represent each cleaned .wav file in Python. We used several </span><span class="c2 c12">librosa </span><span class="c5 c2">functions to extract the following features from these 1-D array representations. The following features are categorized as frequency spectrum features and as decomposition features. Frequency spectrum features are related to the frequency spectrum of the audio signal. Decomposition features are related to decomposing the audio signal into multiple elements.</span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3"><span class="c2">-</span><span class="c4 c2 c9">Frequency Spectrum Features</span></p><ul class="c20 lst-kix_git6csdi9890-0 start"><li class="c3 c22"><span class="c2">Mel-frequency cepstral coefficients (</span><span class="c4 c2">MFCCs</span><span class="c5 c2">) are calculated by taking the power spectrum of the signal, converting the spectrum into the mel scale which is more proportionate to human perception of pitch than the frequency, binning the mel power spectrum, and performing a couple of mathematical operations on the bins. One hyperparameter we can tune in the future is the number of bins we use to divide the mel power spectrum (the default is 40 bins). The MFCCs are commonly used for audio classification and speech recognition. </span></li></ul><p class="c3 c23"><span class="c2">The MFCCs are outputted in descending order of significance; in other words, the first MFCC is more important than the second which is more important than the third and so on. The standard practice for speech recognition is to take the first 13 MFCCs (</span><span class="c4 c2 c27"><a class="c17" href="https://www.google.com/url?q=https://arxiv.org/ftp/arxiv/papers/1305/1305.1145.pdf&amp;sa=D&amp;ust=1604597826972000&amp;usg=AOvVaw2yU70aHXugQIATwU1_eY6I">https://arxiv.org/ftp/arxiv/papers/1305/1305.1145.pdf</a></span><span class="c5 c2">&nbsp;). We need to figure out how many MFCCs we need to take &nbsp;for our purposes to make the best instrument classification model while minimizing the number of total features.</span></p><ul class="c20 lst-kix_git6csdi9890-0"><li class="c3 c22 c18"><span class="c2">The </span><span class="c4 c2">zero-crossing rate</span><span class="c5 c2">&nbsp;is the frequency at which the audio signal crosses the zero amplitude in the time domain. The zero-crossing rate indicates how high or low the dominant frequency is.</span></li><li class="c3 c22 c18"><span class="c2">The </span><span class="c4 c2">root mean square</span><span class="c5 c2">&nbsp;of an audio signal is the square root of the mean of the squared data points in the time domain. The root mean square indicates the general size of the amplitude of the audio signal.</span></li><li class="c3 c22 c18"><span class="c2">The </span><span class="c4 c2">spectral centroid</span><span class="c5 c2">&nbsp;is the centroid of the power spectrum (frequency domain). The spectral centroid indicates how high or low the average frequency of the audio signal is.</span></li><li class="c3 c22 c18"><span class="c2">The </span><span class="c4 c2">spectral bandwidth</span><span class="c5 c2">&nbsp;is the width of the power spectrum (frequency domain) where the power spectrum density is greater than half the maximum power spectrum density. A hyperparameter that can be tuned in the future is the order at which the spectral bandwidth is computed (default is second order).</span></li><li class="c3 c22 c18"><span class="c2">The </span><span class="c4 c2">spectral flatness</span><span class="c2 c5">&nbsp;indicates how similar an audio signal is to white noise. The spectral flatness is on a range between 0 and 1. The closer the spectral flatness is to 1, the more similar the audio signal is to white noise.</span></li><li class="c3 c22 c18"><span class="c2">The </span><span class="c4 c2">spectral roll-off</span><span class="c5 c2">&nbsp;is the frequency where a certain percentage of the power spectrum lies at or below the spectral roll-off frequency. The spectral roll-off indicates how high or low the typical frequency of the audio signal is. A hyperparameter we can tune in the future is the percentage of the power spectrum that should be at or below the spectral roll-off (default is 85%).</span></li><li class="c3 c22 c18"><span class="c2">The spectral energy of each of the 12 musical pitches of an octave will be referred to as the </span><span class="c4 c2">12 chroma features</span><span class="c5 c2">. </span></li><li class="c3 c22 c18"><span class="c2">The 12 chroma features can be projected on a 6-D basis to yield </span><span class="c2 c4">six tonal centroid features</span><span class="c5 c2">: two coordinates representing a perfect fifth chord, two coordinates representing a minor third chord, and two coordinates representing a major third chord. </span></li><li class="c3 c22 c18"><span class="c5 c2">The spectrogram is a visual representation of how the signal frequencies vary over time. We did not use a spectrogram as a feature in the first version of our model, because there are simply too many data points within a spectrogram to calculate with our current computing resources. However, several of the features we did use are related to spectrograms. For example, MFCCs can be derived from spectrograms. </span></li></ul><p class="c3 c18 c23"><span class="c2">Another feature derived from spectrograms are the </span><span class="c4 c2">spectral contrast features</span><span class="c5 c2">. The spectrogram can be split into subbands. The difference between the peak energy and valley energy of each subband is the spectral contrast of that subband. Thus, the more subbands we divide the spectrogram into, the more spectral contrast features we have. The number of subbands is another hyperparameter we can tune in the future (default is six subbands).</span></p><p class="c3 c23 c18 c15"><span class="c5 c2"></span></p><p class="c3 c18"><span class="c2">-</span><span class="c9 c4 c2">Decomposition Features</span></p><ul class="c20 lst-kix_git6csdi9890-0"><li class="c3 c18 c22"><span class="c2">An audio signal can be decomposed into a waveform representing the harmonic element of the signal and a waveform representing the percussive element of the signal. Three features can be derived from this phenomenon. The </span><span class="c4 c2">harmonic component</span><span class="c2">&nbsp;is the mean absolute amplitude of the harmonic element, and the </span><span class="c4 c2">percussive component</span><span class="c2">&nbsp;is the mean absolute amplitude of the percussive element. The </span><span class="c4 c2">harmonic-percussive ratio</span><span class="c5 c2">&nbsp;is the ratio between the harmonic and percussive components.</span></li></ul><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3"><span class="c2">It is important to note that when we extracted each frequency spectrum features, the </span><span class="c2 c12">librosa</span><span class="c5 c2">&nbsp;functions we used actually split the audio signal into many overlapping time frames and outputted an array of features (one for each time frame). We consolidated this format by taking the arithmetic mean of each array. For example, the final zero-crossing rate was the arithmetic mean of the zero-crossing rates for each of the time frames.</span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3"><span class="c2">After extracting the features, we stored them in a </span><span class="c2 c12">pandas </span><span class="c5 c2">DataFrame for easy access when training and testing the models.</span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3"><span class="c4 c10">Methods Part 2: Algorithms and Tuning Hyperparamters</span></p><p class="c3"><span class="c5 c2">We used a random forest algorithm to actually predict which instrument family each sound bites belong to. In order to tune our hyperparameters, we created a graph of the result F1 score for a range of values for each hyperparameter. A higher F1 score is desired. We chose the values for the hyperparameters at the elbow point of the graph. Using this method for all the hyperparameters, we were able to tune our hyperparameters. We are also exploring the result from using Convolutional Neural Networks.</span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c16 c15"><span class="c1"></span></p><p class="c16 c15"><span class="c1"></span></p><p class="c16"><span class="c1">Methods Part 3: Feature Selection</span></p><p class="c16"><span class="c5 c2">Due to the number of features we are extracting from each individual audio file, it is expected that some features are more important than others for classification and that certain features contain overlapping information. For these reasons, we employed a few feature selection algorithms to detect the most important features and reduce the dimensionality of the dataset.</span></p><p class="c16"><span class="c2">-</span><span class="c9 c4 c2">Forward Selection/Backwards Elimination</span></p><ul class="c20 lst-kix_u94b9dfyyvn-0 start"><li class="c14"><span class="c5 c2">One way to select features is based on how likely it is that a feature influenced the outcome of a model. In this method, the data is fitted with a least squares regression model. The p-values of the t test statistics for each coefficient of the model are then calculated. Each p-value represents the probability that a feature has no correlation with the label. Thus, lower p-values indicate features that significantly affect the outcome. Both forward selection and backwards elimination select a feature based on whether its corresponding p-value meets a certain threshold, denoted as the significance level.</span></li><li class="c14"><span class="c5 c2">In forward selection, a model is fitted for every possible feature, and the feature with the lowest p-value is selected. Then, a model is fitted for every remaining feature with the previously selected features, and the feature with the lowest p-value among the models is again included. This continues until the minimum p-value is greater than the significance level.</span></li><li class="c14"><span class="c5 c2">In backwards elimination, a full model is fitted with all the features, and the feature with the highest p-value is excluded. Then, the model is fit with the remaining features, and this process continues until the maximum p-value is less than the significance level.</span></li><li class="c14"><span class="c5 c2">Implementation of both of these methods resulted in the number of features being reduced from 46 to 30. In both cases, MFCCs and other spectrogram features were found to be very important, whereas Chromagram features and tonal centroids were somewhat less important.</span></li></ul><img src="midtermReport1.png"><p class="c16"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 182.67px;"><img alt="" src="images/image4.png" style="width: 624.00px; height: 182.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c16"><span class="c2">-</span><span class="c9 c4 c2">Feature Selection with LASSO Regression</span></p><ul class="c20 lst-kix_9acpcmg5880u-0 start"><li class="c14"><span class="c2">Another method to reduce the number of features is to fit the data into a Lasso regression model and pick features for which the coefficients are nonzero. Lasso solves the least squares problem under the constraint of a regularization term that is proportional to the l</span><span class="c2 c31">1</span><span class="c5 c2">&nbsp;norm of the coefficient vector. The coefficient of the regularization term, denoted alpha, is optimized using cross validation. This process tends to produce sparse coefficients, so using this method results in a drastically smaller feature set, making it useful for feature selection.</span></li><li class="c14"><span class="c5 c2">Specifically, picking only features with nonzero coefficients resulted in 19 of the original 46 features being used. The results are consistent with forward selection/backwards elimination in that MFCCs and Spectral features were found to be most important.</span></li></ul><p class="c16"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 616.00px; height: 112.00px;"><img alt="" src="images/image1.png" style="width: 616.00px; height: 112.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c16"><span class="c2">-</span><span class="c9 c4 c2">PCA</span></p><ul class="c20 lst-kix_clvlb54q34iz-0 start"><li class="c14"><span class="c5 c2">We used Principal Component Analysis to detect the relationships between features. PCA redefines the dimensionality of the data by choosing principal directions with the highest variance. Thus, more information is encoded in fewer dimensions, and the number of features needed can be significantly reduced. By using PCA on the entire feature set, 99% of explained variance can be encoded in just the first 3 components. By choosing 8 components, the accuracy of the Random Forests model is within 2% of the original accuracy without PCA. PCA was also used on the reduced feature set created by the previous methods, which produced similar results.</span></li></ul><p class="c16"><span class="c2">-</span><span class="c9 c4 c2">Overall Results (maybe put in results)</span></p><p class="c16"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 523.00px; height: 224.00px;"><img alt="" src="images/image3.png" style="width: 523.00px; height: 224.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><ul class="c20 lst-kix_a60e7wsgu2v7-0 start"><li class="c14"><span class="c25 c2 c24">In each case, the feature set of the data could be reduced significantly without affecting the overall accuracy of the Random Forests model. With PCA, a relatively low number of principal components are required to achieve similarly accurate results.</span></li></ul><p class="c3 c15"><span class="c1"></span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3"><span class="c4 c10">Results</span></p><p class="c3"><span class="c5 c2">We found remarkably high accuracy using just the algorithms stated previously. At present, we are investigating avenues that could perhaps allow our model to generalize to other kinds of input (perhaps noisy data). We have almost 99% raw labelling accuracy from the given data. Our confusion matrix for the test data is as follows:</span></p><p class="c3"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 412.00px;"><img alt="" src="images/image2.png" style="width: 624.00px; height: 412.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3"><span class="c5 c2">The mislabellings appear trivial compared to the rest and the data set, while not uniform, is somewhat balanced.</span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3"><span class="c4 c10">Discussion</span></p><p class="c3"><span class="c5 c2">The most important features are those that come from MFCCs and spectrograms in accuracy as compared to chromagram features and tonal centroid features. The accuracy suffers more when MFCCs and spectrograms are removed as compared to chromagram and tonal centroid features. Therefore, we can reduce the use of chromagram and tonal centroid features while keeping a high accuracy rate. We have currently identified a random forest classifier from sklearn as the best algorithm to classify our data into their instruments. The random forest classifier obtained an accuracy rate of 0.990. We also used a naive bayes classifier and a MLP neural network classifier which both performed worse with accuracy rates of 0.472 and 0.790 respectively. Another algorithm we can use in the future is a CNN applied to the MFCCs, spectrograms, and chromograms; each of these features provide an image which CNNs specialize in analyzing. A new feature we could use is dynamic range which is the ratio of the largest and smallest volume that a wav file contains. Additionally, we plan on using AWS in the future for greater computing power, so we can train and test our model on larger sets of data instead of a smaller subset. This will allow us to come up with a final product that can better classify instruments based on their wav files. We can also tune the hyperparameters for our features and the algorithms to improve our results going forward.</span></p><p class="c3 c15"><span class="c4 c8"></span></p><p class="c3"><span class="c8 c4">References</span></p><ol class="c20 lst-kix_pb2bpaedok4i-0 start" start="1"><li class="c3 c22"><span class="c2 c24">Sharma, G., Umapathy, K., &amp; Krishnan, S. (2020). Trends in audio signal feature extraction methods. </span><span class="c2 c7">Applied Acoustics</span><span class="c25 c2 c24">, 158, 107020. doi:10.1016/j.apacoust.2019.107020</span></li><li class="c3 c22"><span class="c25 c2 c24">Loughran, R., Walker, J., O&#39;Neill, M., &amp; O&#39;Farrell, M. (2008). The Use of Mel-frequency Cepstral Coefficients in Musical Instrument Identification. </span></li><li class="c3 c22"><span class="c2 c24 c25">Deperlioglu, O. (2018). Classification of Phonocardiograms with Convolutional Neural Networks. 9. </span></li><li class="c3 c22 c18"><span class="c2 c19">Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Douglas Eck, Karen Simonyan, and Mohammad Norouzi.(2017). &quot;Neural Audio Synthesis of Musical Note &nbsp; with WaveNet Autoencoders.&quot;</span></li><li class="c3 c22 c18"><span class="c19 c2">Shrawankar, U., &amp; Thakare, V. M. (2013). Techniques for Feature Extraction In Speech Recognition System : A Comparative Study. </span></li></ol><p class="c3 c18 c15"><span class="c19 c2"></span></p><p class="c3"><span class="c2">NSynth dataset:</span><span class="c2">&nbsp;</span><span class="c4 c2 c27"><a class="c17" href="https://www.google.com/url?q=https://magenta.tensorflow.org/datasets/nsynth%23description&amp;sa=D&amp;ust=1604597826987000&amp;usg=AOvVaw2kYeuNlM7YwA952hED6vHc">https://magenta.tensorflow.org/datasets/nsynth#description</a></span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3"><span class="c2">Librosa:</span><span class="c2">&nbsp;</span><span class="c4 c2 c27"><a class="c17" href="https://www.google.com/url?q=https://librosa.org/doc/latest/index.html&amp;sa=D&amp;ust=1604597826988000&amp;usg=AOvVaw1gCOA4mj5rNbs7kEKgj0i-">https://librosa.org/doc/latest/index.html</a></span></p><p class="c3 c18 c15"><span class="c5 c2"></span></p><p class="c3 c15"><span class="c5 c2"></span></p><p class="c3 c15"><span class="c5 c2"></span></p></div></body></html>
